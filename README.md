# Google-ngarms
This repository is dedicated to a Big Data project on Google Books Ngrams.

The central aim of this project is to apply the skills and knowledge of `Big Data` to the Google `Ngrams` dataset, a remarkable creation by Google's research team. This dataset has been meticulously crafted through the analysis of content derived from Google Books, encompassing a vast repository that encapsulates around 4% of the entirety of printed books throughout history. This extensive collection spans across epochs, beginning in the 1800s and stretching into the 2000s. 

<img align="right" alt="Coding" width="350" src="https://github.com/amirkiaml/media/blob/main/Google%20ngrams.png">

The primary focus is on the proficient `loading`, `filtering`, and dynamic `visualization` of an expansive `real-world` dataset. This task will be carried out within the realm of a `cloud-based` distributed computing environment, making optimal use of prominent technologies including `Hadoop`, `Spark`, `Hive`, and the `S3 filesystem`, all of which are integral components of the process. A comprehensive and professionally structured `report` is provided at the end, encapsulating a concise overview of our findings.

This project contains two files:
- [File 1](https://github.com/amirkiaml/Google-ngarms/blob/main/Introduction%20and%20Q1-Q3%20Walkthrough.pdf): a PDF of the steps taken to set up AWS S3 Clusters and Spark in the cloud
- [File 2](https://github.com/amirkiaml/Google-ngarms/blob/main/Amirhossein_Kiani_Big_Data_Wrangling.ipynb): a Jupyter notebook containing big data wrangling codes on the Google Ngrams dataset.
