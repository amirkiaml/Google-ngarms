{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3a5bc6",
   "metadata": {},
   "source": [
    "# Big Data Wrangling with Google Books Ngrams\n",
    "\n",
    "#### Name: Amirhossein Kiani\n",
    "#### Email: amkoxia@gmail.com\n",
    "#### Date: July 23, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d6d82",
   "metadata": {},
   "source": [
    "## Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fd6fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff68bebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1690152117275_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-9-233.us-east-2.compute.internal:20888/proxy/application_1690152117275_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-2-8.us-east-2.compute.internal:8042/node/containerlogs/container_1690152117275_0005_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f2ee8e594d0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396dc864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- ----------\n",
      "aws-cfn-bootstrap          2.0\n",
      "beautifulsoup4             4.9.3\n",
      "boto                       2.49.0\n",
      "click                      8.1.3\n",
      "docutils                   0.14\n",
      "jmespath                   1.0.1\n",
      "joblib                     1.2.0\n",
      "lockfile                   0.11.0\n",
      "lxml                       4.9.2\n",
      "mysqlclient                1.4.2\n",
      "nltk                       3.8\n",
      "nose                       1.3.4\n",
      "numpy                      1.20.0\n",
      "pip                        20.2.2\n",
      "py-dateutil                2.2\n",
      "pystache                   0.5.4\n",
      "python-daemon              2.2.3\n",
      "python37-sagemaker-pyspark 1.4.2\n",
      "pytz                       2022.7\n",
      "PyYAML                     5.4.1\n",
      "regex                      2021.11.10\n",
      "setuptools                 28.8.0\n",
      "simplejson                 3.2.0\n",
      "six                        1.13.0\n",
      "tqdm                       4.64.1\n",
      "wheel                      0.29.0\n",
      "windmill                   1.6\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5ef840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.5\n",
      "  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.0.5) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==1.0.5) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.5 python-dateutil-2.8.2\n",
      "\n",
      "Collecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.7/site-packages (from matplotlib==3.1.1) (1.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./tmp/1690158696811-0/lib/python3.7/site-packages (from matplotlib==3.1.1) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.1) (1.13.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: cycler, pyparsing, typing-extensions, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.4.4 matplotlib-3.1.1 pyparsing-3.1.0 typing-extensions-4.7.1\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "# install data science and plotting packages\n",
    "\n",
    "sc.install_pypi_package(\"pandas==1.0.5\") \n",
    "sc.install_pypi_package(\"matplotlib==3.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b166e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbb9db",
   "metadata": {},
   "source": [
    "## Q1-3\n",
    "The following steps have been carried out separately, with the steps detaileed in a PDF document available in the repository.\n",
    "\n",
    "**Q1:** Spin up a new EMR cluster on AWS for using Spark and EMR notebooks - follow the same instructions as for the Spark Lab.\n",
    "\n",
    "**Q2:** Connect to the head node of the cluster using SSH.\n",
    "\n",
    "**Q3:** Copy the data folder from the S3 bucket directly into a directory on the Hadoop File System (HDFS) named\n",
    "\n",
    "I did the following in the order that they appear, in my Git Bash. Screenshots are attached to the deliverable folder:\n",
    "\n",
    "- cd '<the directory where the .pem file is located>'\n",
    "\n",
    "- ssh -i amirkia_hadoop.pem -L 9995:localhost:9443 hadoop@ec2-3-133-154-124.us-east-2.compute.amazonaws.com\n",
    "\n",
    "- hadoop distcp s3://brainstation-dsft/eng_1M_1gram.csv /user/hadoop/eng_1M_1gram\n",
    "\n",
    "- Open https://localhost:9995 in my browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f026a",
   "metadata": {},
   "source": [
    "## Q4. Working with Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda9f7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read from S3 directly\n",
    "books = spark.read.csv('s3://brainstation-dsft/eng_1M_1gram.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c70451",
   "metadata": {},
   "source": [
    "### 4. a. Describe the dataset (examples include size, shape, schema) in pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49b057",
   "metadata": {},
   "source": [
    "`df.printSchema()` give us the following information for each column in the DataFrame:\n",
    "- Column names\n",
    "- Data type of each column\n",
    "- Whether the column can contain null values ('nullable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f6ebb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- frequency: integer (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      " |-- books: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "books.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbb1a5",
   "metadata": {},
   "source": [
    "We have 5 columns: `token`, `year`, `frequency`, `pages`, and `books`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "045a831e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261823225"
     ]
    }
   ],
   "source": [
    "books.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5e080",
   "metadata": {},
   "source": [
    "We have 261,823,225 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a2ec83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----+-----+\n",
      "|    token|year|frequency|pages|books|\n",
      "+---------+----+---------+-----+-----+\n",
      "|inGermany|1927|        2|    2|    2|\n",
      "|inGermany|1929|        1|    1|    1|\n",
      "|inGermany|1930|        1|    1|    1|\n",
      "|inGermany|1933|        1|    1|    1|\n",
      "|inGermany|1934|        1|    1|    1|\n",
      "|inGermany|1935|        1|    1|    1|\n",
      "|inGermany|1938|        5|    5|    5|\n",
      "|inGermany|1939|        1|    1|    1|\n",
      "|inGermany|1940|        1|    1|    1|\n",
      "|inGermany|1942|        2|    2|    2|\n",
      "|inGermany|1944|        2|    2|    2|\n",
      "|inGermany|1946|        2|    2|    2|\n",
      "|inGermany|1947|        3|    3|    2|\n",
      "|inGermany|1948|        1|    1|    1|\n",
      "|inGermany|1949|        1|    1|    1|\n",
      "|inGermany|1952|        1|    1|    1|\n",
      "|inGermany|1956|        1|    1|    1|\n",
      "|inGermany|1957|        2|    2|    2|\n",
      "|inGermany|1959|        1|    1|    1|\n",
      "|inGermany|1960|        3|    3|    3|\n",
      "+---------+----+---------+-----+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "books.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370869f9",
   "metadata": {},
   "source": [
    "### 4.b. Create a new DataFrame from a query using Spark SQL, filtering to include only the rows where the token is \"data\" and describe the new dataset\n",
    "Create a new DataFrame from a query using Spark SQL, filtering to include only the rows where the token is \"data\" and describe the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25c981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|token|\n",
      "+-----+\n",
      "| data|\n",
      "| data|\n",
      "| data|\n",
      "| data|\n",
      "| data|\n",
      "+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Register the dataframe as a view\n",
    "book.createOrReplaceTempView(\"books\")    # Creates or replaces a local temporary view with this DataFrame.\n",
    "\n",
    "# Execute a SQL query\n",
    "spark.sql(\"SELECT token FROM books WHERE token == 'data'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99969711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'data' in the token column: 316"
     ]
    }
   ],
   "source": [
    "# Count the number of rows that have 'data' in the 'token' column\n",
    "count = spark.sql(\"SELECT COUNT(*) FROM books WHERE token == 'data'\").collect()[0][0]\n",
    "\n",
    "# Check the result\n",
    "print(\"Number of rows with 'data' in the token column:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf5493",
   "metadata": {},
   "source": [
    "### 4.c. Write the filtered data back to a directory in the HDFS from Spark using `df.write.csv()`. Be sure to pass the `header=True` parameter and examine the contents of what you've written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e3bef93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered = spark.sql(\"SELECT token, year, COUNT(*) AS count \\\n",
    "                     FROM books \\\n",
    "                     WHERE token == 'data' \\\n",
    "                     GROUP BY token, year \\\n",
    "                     ORDER BY count DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00731d7a",
   "metadata": {},
   "source": [
    "This query filters the dataframe to only include rows where the value in the `token` column is equal to 'data'. Then the query groups the remaining rows by both the `token` and `year` columns and counts the number of rows in each group. The resulting dataframe is sorted in descending order using the `count` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0110686",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb12595",
   "metadata": {},
   "source": [
    "Now we write the contents of the `filtered` dataframe to a CSV file located at `/user/hadoop/filtered.csv` in the HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59f2c1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered.write.csv(\"/user/hadoop/filtered.csv\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db8e0e",
   "metadata": {},
   "source": [
    "### Q5. Collect the contents of the directory into a single file on the local drive of the head node using `getmerge` and move this file into a S3 bucket in personal account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be905998",
   "metadata": {},
   "source": [
    "I created an S3 bucket named 'amirkiabucket' and ran the below code in the GitBash terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71597e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hadoop fs -getmerge /user/hadoop/filtered.csv /home/hadoop/filtered.csv\n",
    "# aws s3 cp filtered.csv s3://amirkiabucket/filtered.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a58f8f",
   "metadata": {},
   "source": [
    "The first line merges all files in the `/user/hadoop/filtered.csv` directory in HDFS into a single file, and save the resulting file to `/home/hadoop/filtered.csv` on the local file system. The second line copies the file to the S3 bucket (amirkiabucket)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f9d06",
   "metadata": {},
   "source": [
    "### Q6. On your local machine (or on AWS outside of Spark) in python, read the CSV data from the S3 folder into a pandas DataFrame (You will have to research how to read data into pandas from S3 buckets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57a8834a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|token|year|count|\n",
      "+-----+----+-----+\n",
      "| data|1764|    1|\n",
      "| data|1817|    1|\n",
      "| data|1840|    1|\n",
      "| data|1846|    1|\n",
      "| data|1855|    1|\n",
      "| data|1769|    1|\n",
      "| data|1963|    1|\n",
      "| data|1627|    1|\n",
      "| data|1965|    1|\n",
      "| data|2002|    1|\n",
      "| data|1767|    1|\n",
      "| data|1798|    1|\n",
      "| data|1968|    1|\n",
      "| data|1805|    1|\n",
      "| data|1874|    1|\n",
      "| data|1775|    1|\n",
      "| data|1945|    1|\n",
      "| data|1753|    1|\n",
      "| data|1825|    1|\n",
      "| data|1834|    1|\n",
      "+-----+----+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from S3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the S3 bucket and file path\n",
    "bucket_name = \"amirkiabucket\"\n",
    "file_name = \"filtered.csv\"\n",
    "file_path = f\"s3a://{bucket_name}/{file_name}\"\n",
    "\n",
    "# Read the CSV file from S3 and create a DataFrame\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179a744",
   "metadata": {},
   "source": [
    "### Q7. Plot the number of occurrences of the token (the frequency column) of data over the years using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0afcf620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|token|count|\n",
      "+-----+-----+\n",
      "| data|  316|\n",
      "+-----+-----+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by the 'token' column and count occurrences\n",
    "token_counts = df.groupBy(\"token\").count().orderBy(\"token\")\n",
    "\n",
    "# Show the token counts\n",
    "token_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "216b3053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1690152117275_0004/container_1690152117275_0004_01_000001/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 86, in toPandas\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1690152117275_0004/container_1690152117275_0004_01_000001/pyspark.zip/pyspark/sql/pandas/utils.py\", line 40, in require_minimum_pandas_version\n",
      "    \"your version was %s.\" % (minimum_pandas_version, pandas.__version__)\n",
      "ImportError: Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# filter for the \"data\" token\n",
    "counts = df.filter(df.token == \"data\")\n",
    "\n",
    "# group by year and sum the counts\n",
    "counts = counts.groupBy(\"year\").sum(\"count\")\n",
    "\n",
    "# convert to Pandas DataFrame and sort by year\n",
    "counts_pd = counts.toPandas().sort_values(\"year\")\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_counts_pd[\"year\"], data_counts_pd[\"sum(count)\"], marker='o')\n",
    "plt.title(\"Number of occurrences of 'data' over the years\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0addabe2",
   "metadata": {},
   "source": [
    "**NOTE:** I re-ran everything but my connection was unstable and since I was getting charged for my EMR cluster, I thought to shut this down. This should work now that I have installed the relevant version of Pandas earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccd1f0",
   "metadata": {},
   "source": [
    "### Q8. Compare Hadoop and Spark as distributed file systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f860c",
   "metadata": {},
   "source": [
    "#### a) What are the advantages/ differences between Hadoop and Spark? List two advantages for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6344e",
   "metadata": {},
   "source": [
    "#### Haddop:\n",
    "**Advanteges:**\n",
    "\n",
    "- Haddop offers a cost-efficient model, being free to use, and utilizing inexpensive commodity hardware.\n",
    "- The sytem's high scalability allows it to efficiently distribute large datasets across multiple low-cost machines in a clustr, enabling parallel processing.\n",
    "\n",
    "#### Spak:\n",
    "**Advantages:**\n",
    "\n",
    "- Spark's remarkabl speed, up to 100 timess faster than Haddop for large-scale data processing, is achieved through in-memory computng and various optimizations.\n",
    "\n",
    "- Spak comes with a comprhensive set of higher-level libraries, supporting SQL queries, streaming data, machne learning, and graph processing, making it more verstatile for diverse data tasks.\n",
    "\n",
    "Also, Spark outperforms Haddop due to its utilisation of RAM instad of reading and writing intermediate data to disks. In contrast, Haddop processes data in batches through MapReduce, which can be comparativly slower. Haddop's advantage lies in its lower cost as it effciently operates with various disk storage types for data processing. Spark's in-memory approach, though fast, may requir more memory resources and potentially higher infrastucture costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56093e3c",
   "metadata": {},
   "source": [
    "#### b) Explain how the HDFS stores the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319a05a",
   "metadata": {},
   "source": [
    "The Hadoop Distributed File System (HDFS) is the primary storage system used by Hadoop. It divides files into blocks and stores each block on a DataNode, which are worker nodes in the Hadoop cluster. The NameNode is the central metadata server in the HDFS. It stores information about the location of the data blocks and which DataNodes they are stored on. Applications can access data stored in the HDFS through the Hadoop Distributed File System API, which provides a simple interface for reading and writing files stored in the HDFS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
